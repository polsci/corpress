{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from corpress.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpress\n",
    "\n",
    "> Create a text corpus from a WordPress site using the WordPress API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geoff Ford  \n",
    "[https://geoffford.nz](https://geoffford.nz)  \n",
    "Version 1.0.2 \n",
    "\n",
    "[Corpress documentation](https://geoffford.nz/corpress)  \n",
    "\n",
    "Corpress (Cor from Corpus, Press from WordPress) provides a simple approach to retrieve posts or pages from a WordPress site's [REST API](https://developer.wordpress.org/rest-api/) and create a corpus (i.e. a data-set of texts). Corpress provides an efficient and standardized way to collect text data from WordPress sites, avoiding the need for customized scrapers. Not all WordPress sites provide access to the REST API, but many do.  \n",
    "\n",
    "I'm a political scientist, who applies corpus linguistics and digital methods in [my research](https://geoffford.nz) and I'm releasing this with academic researchers in mind. This tool is intended for academic research. Please cite the Github repository for [Corpress](https://github.com/polsci/corpress) if you use it in your research.\n",
    "\n",
    "Corpress attempts to detect a REST API endpoint from a website URL for [posts](https://developer.wordpress.org/rest-api/reference/posts/#list-posts) (default) and [pages](https://developer.wordpress.org/rest-api/reference/pages/#list-pages), then downloads JSON from the API, and then processes the JSON to create a corpus. You can create a corpus in:\n",
    "1. 'txt' format: texts are saved in separate .txt files, compatible with common corpus linguistics tools, like AntConc. An optional meta-data file can be output with the link to each file, title, and date; or      \n",
    "2. 'csv' format: meta-data and text is saved in a single CSV file.\n",
    "\n",
    "I've used [nbdev](https://nbdev.fast.ai/) to develop this library, which uses a Jupyter notebooks to develop code, [documentation](https://geoffford.nz/corpress), code examples and tests. If you want to contribute, you will need to clone the [Github repo](https://github.com/polsci/corpress) and [setup nbdev](https://nbdev.fast.ai/getting_started.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "This library was developed through my research on these projects:  \n",
    "* [Mapping LAWS project: Issue Mapping and Analysing the Lethal Autonomous Weapons Debate](https://mappinglaws.net/) (Funded by Royal Society of New Zealand's Marsden Fund, Grant 19-UOC-068)   \n",
    "* [Into the Deep: Analysing the Actors and Controversies Driving the Adoption of the World’s First Deep Sea Mining Governance](https://miningthesea.net/) (Funded by Royal Society of New Zealand's Marsden Fund, Grant 22-UOC-059)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Add in a way to zip a txt format corpus\n",
    "- Sort out encoding - currently assumes UTF-8 all the way. \n",
    "- Add checks on JSON save path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install corpress\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* There are good reasons not to collect and/or distribute corpora and it is the end-user's responsibility to use this software in an ethical way.  \n",
    "* Depending on the nature of the texts collected, what you are doing when analyzing the texts, and how you disseminate your research, it may be appropriate to process the texts further (e.g. to remove personally identifying information).  \n",
    "* Not all Wordpress sites make the REST API accessible. See [example output when there is no REST API available](#no-rest-api-available).\n",
    "* It is possible the API data may differ from what is visible online. You should check the texts in your corpus to make sure you have what you expect!\n",
    "* Corpress will exit with appropriate logging information if an API endpoint is not found, not accessible or returns unexpected data. Just read what it returns.  \n",
    "* Collecting data uses energy and server resources. It is your responsibility to [set an appropriate User Agent](#set-an-appropriate-user-agent) and seconds between requests to the API to be thoughtful and respectful in your use of this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [corpress function](https://geoffford.nz/corpress/core#corpress) is the intended way to invoke Corpress and create a corpus. Other functions are relevant if you just want to get the API endpoint or download the JSON data. If you want the data in a different format, you could just generate the CSV and then convert that to whatever format you need.   \n",
    "\n",
    "Corpress is intentionally verbose in terms of log output. This is helpful to record and understand the process of collecting the data.\n",
    "\n",
    "Most WordPress sites don't have more than 100s to 1000s of posts. Using a Jupyter Notebook could be helpful to view and capture the log data from running Corpress and scope the corpus.  \n",
    "\n",
    "Here's a step-by-step description, with discussion of the key functionality.  \n",
    "\n",
    "First import the corpress function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpress.core import corpress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to need to set a few arguments for corpress. The corpress function is [documented in full here](https://geoffford.nz/corpress/core#corpress). Here I'm breaking it down and showing an example.\n",
    "\n",
    "* `url`: Set the URL of the WordPress website, Corpress will try to determine the endpoint from this.  \n",
    "* `endpoint_type`: Do you want 'posts' or 'pages'. If you want both, see the note on [collecting both posts and pages](#collecting-both-posts-and-pages).   \n",
    "* `corpus_format`: How do you want your corpus saved? 'txt' is a directory of txt files, 'csv' is a single CSV with meta-data and text.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.adho.org/'\n",
    "endpoint_type = 'posts'\n",
    "corpus_format = 'txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup where and how to save the data. Corpress will try and create directory paths if they don't exist.\n",
    "* `json_save_path` (required): Specify the directory where Corpress will save the JSON data. Note: you should set a new path for every new Wordpress site you collect.   \n",
    "* `corpus_save_path`: Required for 'txt' corpus format, this is where the .txt files will be saved. Set as `None` or ommit if using 'csv' format.  \n",
    "* `csv_save_file`: \n",
    "    * For 'txt' corpus format this is optional. This provides a way to export meta-data (date, title, link to text etc) for each text in the corpus.\n",
    "    * For 'csv' corpus format this is required. This specifies the file where the meta-data and text will be saved.  \n",
    "* `include_title_in_text`: Depending on the data you are collecting and what you want to do with it, you can save the title of the post/page as part of the text or not. This is set to `True` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save_path = '../test_data/example/json/'\n",
    "corpus_save_path = '../test_data/example/txt/'\n",
    "csv_save_file = csv_save_file='../test_data/example/metadata.csv'\n",
    "include_title_in_text = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set how you query the API:\n",
    "* `seconds_between_requests`: By default this is set to one request every 5 seconds. You can't specify less than 1 second. It may be appropiate if you are collecting lots of texts to specify a large number of seconds between requests.    \n",
    "* `headers`: Corpress uses the [Requests](https://requests.readthedocs.io/en/latest/) Python Library for HTTP requests. You can pass headers you want in HTTP requests directly as a `dict`. [See documentation here](https://requests.readthedocs.io/en/latest/user/quickstart/#custom-headers). The most relevant one is to set a [User-Agent header](https://en.wikipedia.org/wiki/User-Agent_header).  See the note below about how to [set an appropriate User-Agent](#set-an-appropriate-user-agent).  \n",
    "* `params`: The [posts](https://developer.wordpress.org/rest-api/reference/posts/#list-posts) and [pages](https://developer.wordpress.org/rest-api/reference/pages/#list-pages) endpoints support a number of parameters. This includes parameters to specify a search term, restrict dates and set the way results are ordered. Set additional parameters as a `dict`. See the Requests library documentation on [passing parameters in URLS](https://requests.readthedocs.io/en/latest/user/quickstart/#passing-parameters-in-urls) to understand this.\n",
    "* `max_pages`: By default Corpress will collect *all* post (or pages). That might not be necessary. Interpret max_pages as the maximum number of successful API requests. The REST API normally returns 10 posts/pages per request, so if you want 100 posts you would set max_pages to 10.  \n",
    "\n",
    "#### Set an appropriate User-Agent\n",
    "\n",
    "Here's a suggested format: \n",
    "`Your Research Project (https://university.edu/webpage)`. See how to set this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_between_requests = 5\n",
    "headers = {'User-Agent': 'Your Research Project (https://university.edu/webpage)'}\n",
    "params = {'search': 'common'} # just comment out or remove this line to collect every post, I've just chosen a word arbitrarily here\n",
    "max_pages = None # collecting all available data, if want less data - set to an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call the `corpress` function and create a corpus. There will be lots of information logged about collecting and processing the texts. When completed it will output a table with a summary of the process and texts collected. This is the same data returned by the `corpress` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 11:21:25 - INFO - Found REST API endpoint link\n",
      "2024-08-23 11:21:25 - INFO - Setting posts route https://adho.org/wp-json/wp/v2/posts\n",
      "2024-08-23 11:21:25 - INFO - Using JSON save path: ../test_data/example/json/\n",
      "2024-08-23 11:21:27 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=1\n",
      "2024-08-23 11:21:27 - INFO - Total pages to retrieve is 3\n",
      "2024-08-23 11:21:34 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=2\n",
      "2024-08-23 11:21:40 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=3\n",
      "2024-08-23 11:21:45 - INFO - Creating corpus in txt format\n",
      "2024-08-23 11:21:45 - INFO - Using corpus save path: ../test_data/example/txt/\n",
      "2024-08-23 11:21:45 - INFO - Creating CSV file for metadata: ../test_data/example/metadata.csv\n",
      "2024-08-23 11:21:45 - INFO - Processing JSON: posts-3.json\n",
      "2024-08-23 11:21:45 - INFO - Processing JSON: posts-2.json\n",
      "2024-08-23 11:21:45 - INFO - Processing JSON: posts-1.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>url</td>\n",
       "      <td>https://www.adho.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>endpoint_url</td>\n",
       "      <td>https://adho.org/wp-json/wp/v2/posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>headers</td>\n",
       "      <td>{'User-Agent': 'Your Research Project (https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>params</td>\n",
       "      <td>{'search': 'common'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get_api_url</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>get_json</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>create_corpus</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>corpus_format</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corpus_save_path</td>\n",
       "      <td>../test_data/example/txt/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>csv_save_file</td>\n",
       "      <td>../test_data/example/metadata.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>corpus_texts_count</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Key                                              Value\n",
       "0                  url                              https://www.adho.org/\n",
       "1         endpoint_url               https://adho.org/wp-json/wp/v2/posts\n",
       "2              headers  {'User-Agent': 'Your Research Project (https:/...\n",
       "3               params                               {'search': 'common'}\n",
       "4          get_api_url                                               True\n",
       "5             get_json                                               True\n",
       "6        create_corpus                                               True\n",
       "7        corpus_format                                                txt\n",
       "8     corpus_save_path                          ../test_data/example/txt/\n",
       "9        csv_save_file                  ../test_data/example/metadata.csv\n",
       "10  corpus_texts_count                                                 29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = corpress(url=url, \n",
    "                  endpoint_type=endpoint_type, \n",
    "                  corpus_format=corpus_format, \n",
    "                  json_save_path=json_save_path, \n",
    "                  corpus_save_path=corpus_save_path, \n",
    "                  csv_save_file=csv_save_file, \n",
    "                  include_title_in_text=include_title_in_text, \n",
    "                  seconds_between_requests=seconds_between_requests, \n",
    "                  headers=headers, \n",
    "                  params=params, \n",
    "                  max_pages=max_pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now preview the data you've collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-12-06</td>\n",
       "      <td>ADHO Adopts Creative Commons License for Its Web Site</td>\n",
       "      <td>https://adho.org/2012/12/06/adho-adopts-creative-commons-license-for-its-web-site/</td>\n",
       "      <td>2012-12-06-post-382-adho-adopts-creative-commons-license-for-its-web-site.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>Apply to be ADHO’s Publications Liaison</td>\n",
       "      <td>https://adho.org/2013/03/28/apply-to-be-adhos-publications-liaison/</td>\n",
       "      <td>2013-03-28-post-366-apply-to-be-adhos-publications-liaison.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-06-23</td>\n",
       "      <td>ADHO Calls for Proposals for New Special Interest Groups</td>\n",
       "      <td>https://adho.org/2013/06/23/adho-calls-for-proposals-for-new-special-interest-groups/</td>\n",
       "      <td>2013-06-23-post-338-adho-calls-for-proposals-for-new-special-interest-groups.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-07-09</td>\n",
       "      <td>Participate in the Joint ADHO and centerNet AGM at Digital Humanities 2013</td>\n",
       "      <td>https://adho.org/2013/07/09/participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013/</td>\n",
       "      <td>2013-07-09-post-408-participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-14</td>\n",
       "      <td>Digital Humanities 2015 to be held in Sydney, Australia</td>\n",
       "      <td>https://adho.org/2013/07/14/digital-humanities-2015-to-be-held-in-sydney-australia/</td>\n",
       "      <td>2013-07-14-post-288-digital-humanities-2015-to-be-held-in-sydney-australia.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  \\\n",
       "8  2012-12-06   \n",
       "7  2013-03-28   \n",
       "6  2013-06-23   \n",
       "5  2013-07-09   \n",
       "4  2013-07-14   \n",
       "\n",
       "                                                                         link  \\\n",
       "8                       ADHO Adopts Creative Commons License for Its Web Site   \n",
       "7                                     Apply to be ADHO’s Publications Liaison   \n",
       "6                    ADHO Calls for Proposals for New Special Interest Groups   \n",
       "5  Participate in the Joint ADHO and centerNet AGM at Digital Humanities 2013   \n",
       "4                     Digital Humanities 2015 to be held in Sydney, Australia   \n",
       "\n",
       "                                                                                                     title  \\\n",
       "8                       https://adho.org/2012/12/06/adho-adopts-creative-commons-license-for-its-web-site/   \n",
       "7                                      https://adho.org/2013/03/28/apply-to-be-adhos-publications-liaison/   \n",
       "6                    https://adho.org/2013/06/23/adho-calls-for-proposals-for-new-special-interest-groups/   \n",
       "5  https://adho.org/2013/07/09/participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013/   \n",
       "4                      https://adho.org/2013/07/14/digital-humanities-2015-to-be-held-in-sydney-australia/   \n",
       "\n",
       "                                                                                             filename  \n",
       "8                       2012-12-06-post-382-adho-adopts-creative-commons-license-for-its-web-site.txt  \n",
       "7                                      2013-03-28-post-366-apply-to-be-adhos-publications-liaison.txt  \n",
       "6                    2013-06-23-post-338-adho-calls-for-proposals-for-new-special-interest-groups.txt  \n",
       "5  2013-07-09-post-408-participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013.txt  \n",
       "4                      2013-07-14-post-288-digital-humanities-2015-to-be-held-in-sydney-australia.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # to display full text in pandas dataframe\n",
    "metadata = pd.read_csv(csv_save_file)\n",
    "metadata = metadata.sort_values('date')\n",
    "metadata[['date', 'link', 'title', 'filename']].head(5) # display first 5 rows of metadata, this is not all the fields available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view a specific text file (if you used the 'txt' format) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADHO Adopts Creative Commons License for Its Web Site\n",
      "\n",
      "The Alliance of Digital Humanities Organizations (ADHO) is pleased to announce that all content on its web site is now available under a Creative Commons Attribution (CC-BY) license. This means that individuals and organizations are welcome to re-use and adapt ADHO’s documents and resources, so long as ADHO is cited as the source. Neil Fraistat, Chair of ADHO’s Steering Committee, notes that “this is one of an ongoing series of actions this year that are being designed to make ADHO resources more open and available to the larger community.”\n",
      " \n",
      "ADHO’s decision to adopt the CC-BY license was prompted by the recognition that through explicitly sharing its work it can have a greater impact, contribute to best practices, and demonstrate its support for open access. Recently the Program Committee for the 2013 Digital Humanities conference  revamped ADHO’s Guidelines for Proposal Authors & Reviewers, making them more inclusive, concrete, and transparent. PC chair Bethany Nowviskie received a request from the organizers of another conference to re-use these guidelines. Prompted by Nowviskie's suggestion, the ADHO Steering Committee determined that not only should the conference guidelines be made freely available, but its entire web site.\n",
      " \n",
      "In adopting a Creative Commons license for its website, ADHO follows suit with several of its existing publications, including Digital Studies/Le Champ Numerique, Digital Humanities Quarterly, and DH Answers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename = '2012-12-06-post-382-adho-adopts-creative-commons-license-for-its-web-site.txt'\n",
    "with open(os.path.join(corpus_save_path, filename), 'r', encoding = 'utf-8') as file:\n",
    "    text = file.read()   \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting both posts and pages\n",
    "\n",
    "If you want to collect both posts and pages, just invoke corpress twice: once with `endpoint_type` set to 'posts' and then with it set to 'pages'.  \n",
    "\n",
    "If you are outputting in the 'txt' corpus format without a metadata file (i.e. `csv_save_file` set to `None` or omitted from the function call), you won't have a problem. The filenames for posts/pages won't conflict.  \n",
    "\n",
    "If you are specifying a `csv_save_file` - either because you are outputting in the 'csv' corpus format or in the 'txt' format and wanting the meta-data - make sure you use a separate `csv_save_file` for 'posts' and 'pages'. You will get two separate files, combining these with a library like [Pandas](https://pandas.pydata.org/), which is installed with Corpress, is trivial. I will leave that for you to Google how to merge two CSV files into one using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No REST API available\n",
    "\n",
    "Here's an example showing what you will see if there no REST API is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-23 11:21:46 - INFO - No REST API endpoint link in markup\n",
      "2024-08-23 11:21:46 - INFO - Guessing posts route based on URL https://www.whitehouse.gov/wp-json/wp/v2/posts\n",
      "2024-08-23 11:21:46 - INFO - Using JSON save path: ../test_data/json/\n",
      "2024-08-23 11:21:46 - INFO - Max pages to retrieve from API is set: 2\n",
      "2024-08-23 11:21:47 - INFO - Downloading https://www.whitehouse.gov/wp-json/wp/v2/posts?page=1\n",
      "2024-08-23 11:21:47 - ERROR - Error downloading page 1 from https://www.whitehouse.gov/wp-json/wp/v2/posts\n",
      "2024-08-23 11:21:47 - ERROR - Status code: 403\n",
      "2024-08-23 11:21:47 - ERROR - It appears that this website does not provide access to the REST API. Exiting.\n",
      "2024-08-23 11:21:47 - ERROR - Error downloading data. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>url</td>\n",
       "      <td>https://www.whitehouse.gov/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>endpoint_url</td>\n",
       "      <td>https://www.whitehouse.gov/wp-json/wp/v2/posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>headers</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>params</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get_api_url</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>get_json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>create_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>corpus_format</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corpus_save_path</td>\n",
       "      <td>../test_data/corpus/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>csv_save_file</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>corpus_texts_count</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Key                                           Value\n",
       "0                  url                     https://www.whitehouse.gov/\n",
       "1         endpoint_url  https://www.whitehouse.gov/wp-json/wp/v2/posts\n",
       "2              headers                                            None\n",
       "3               params                                            None\n",
       "4          get_api_url                                            True\n",
       "5             get_json                                           False\n",
       "6        create_corpus                                           False\n",
       "7        corpus_format                                             txt\n",
       "8     corpus_save_path                            ../test_data/corpus/\n",
       "9        csv_save_file                                            None\n",
       "10  corpus_texts_count                                               0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test a site that has no endpoint\n",
    "result = corpress(url = 'https://www.whitehouse.gov/', \n",
    "                endpoint_type='posts',\n",
    "                corpus_format='txt',\n",
    "                json_save_path = '../test_data/json/', \n",
    "                corpus_save_path = '../test_data/corpus/', \n",
    "                max_pages=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
