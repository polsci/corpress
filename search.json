[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "corpress",
    "section": "",
    "text": "Geoff Ford\nhttps://geoffford.nz/corpress-release\nCorpress documentation\nCorpress (Cor from Corpus, Press from WordPress) provides a simple approach to retrieve posts or pages from a WordPress site’s REST API and create a corpus (i.e. a data-set of texts). Corpress provides an efficient and standardized way to collect text data from WordPress sites, avoiding the need for customized scrapers. Not all WordPress sites provide access to the REST API, but many do.\nI’m a political scientist who applies corpus linguistics and digital methods in my research. I’m releasing Corpress with academic researchers in mind. This tool is intended for academic research. Please cite Corpress if you use it in your research.\nCorpress attempts to detect a REST API endpoint from a website URL for posts (default) and pages, then downloads JSON from the API, and then processes the JSON to create a corpus. You can create a corpus in: 1. ‘txt’ format: texts are saved in separate .txt files, compatible with common corpus linguistics tools, like AntConc. An optional meta-data file can be output with the link to each file, title, and date; or\n2. ‘csv’ format: meta-data and text is saved in a single CSV file.\nI’ve used nbdev to develop this library, which uses a Jupyter notebooks to develop code, documentation, code examples and tests. If you want to contribute, you will need to clone the Github repo and setup nbdev.",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "corpress",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis library was developed through my research on these projects:\n* Mapping LAWS project: Issue Mapping and Analysing the Lethal Autonomous Weapons Debate (Funded by Royal Society of New Zealand’s Marsden Fund, Grant 19-UOC-068)\n* Into the Deep: Analysing the Actors and Controversies Driving the Adoption of the World’s First Deep Sea Mining Governance (Funded by Royal Society of New Zealand’s Marsden Fund, Grant 22-UOC-059)",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "corpress",
    "section": "TODO",
    "text": "TODO\n\nAdd in a way to zip a txt format corpus\nSort out encoding - currently assumes UTF-8 all the way.\nAdd checks on JSON save path.",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "corpress",
    "section": "Install",
    "text": "Install\npip install corpress",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#before-using",
    "href": "index.html#before-using",
    "title": "corpress",
    "section": "Before using",
    "text": "Before using\n\nThere are good reasons not to collect and/or distribute corpora and it is the end-user’s responsibility to use this software in an ethical way.\n\nDepending on the nature of the texts collected, what you are doing when analyzing the texts, and how you disseminate your research, it may be appropriate to process the texts further (e.g. to remove personally identifying information).\n\nNot all Wordpress sites make the REST API accessible. See example output when there is no REST API available.\nIt is possible the API data may differ from what is visible online. You should check the texts in your corpus to make sure you have what you expect!\nCorpress will exit with appropriate logging information if an API endpoint is not found, not accessible or returns unexpected data. Just read what it returns.\n\nCollecting data uses energy and server resources. It is your responsibility to set an appropriate User Agent and seconds between requests to the API to be thoughtful and respectful in your use of this tool.",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "corpress",
    "section": "How to use",
    "text": "How to use\nThe corpress function is the intended way to invoke Corpress and create a corpus. Other functions are relevant if you just want to get the API endpoint or download the JSON data. If you want the data in a different format, you could just generate the CSV and then convert that to whatever format you need.\nCorpress is intentionally verbose in terms of log output. This is helpful to record and understand the process of collecting the data.\nMost WordPress sites don’t have more than 100s to 1000s of posts. Using a Jupyter Notebook could be helpful to view and capture the log data from running Corpress and scope the corpus.\nHere’s a step-by-step description, with discussion of the key functionality.\nFirst import the corpress function.\n\nfrom corpress.core import corpress\n\nYou are going to need to set a few arguments for corpress. The corpress function is documented in full here. Here I’m breaking it down and showing an example.\n\nurl: Set the URL of the WordPress website, Corpress will try to determine the endpoint from this.\n\nendpoint_type: Do you want ‘posts’ or ‘pages’. If you want both, see the note on collecting both posts and pages.\n\ncorpus_format: How do you want your corpus saved? ‘txt’ is a directory of txt files, ‘csv’ is a single CSV with meta-data and text.\n\n\nurl = 'https://www.adho.org/'\nendpoint_type = 'posts'\ncorpus_format = 'txt'\n\nSetup where and how to save the data. Corpress will try and create directory paths if they don’t exist. * json_save_path (required): Specify the directory where Corpress will save the JSON data. Note: you should set a new path for every new Wordpress site you collect.\n* corpus_save_path: Required for ‘txt’ corpus format, this is where the .txt files will be saved. Set as None or ommit if using ‘csv’ format.\n* csv_save_file: * For ‘txt’ corpus format this is optional. This provides a way to export meta-data (date, title, link to text etc) for each text in the corpus. * For ‘csv’ corpus format this is required. This specifies the file where the meta-data and text will be saved.\n* include_title_in_text: Depending on the data you are collecting and what you want to do with it, you can save the title of the post/page as part of the text or not. This is set to True by default.\n\njson_save_path = '../test_data/example/json/'\ncorpus_save_path = '../test_data/example/txt/'\ncsv_save_file = csv_save_file='../test_data/example/metadata.csv'\ninclude_title_in_text = True\n\nSet how you query the API: * seconds_between_requests: By default this is set to one request every 5 seconds. You can’t specify less than 1 second. It may be appropiate if you are collecting lots of texts to specify a large number of seconds between requests.\n* headers: Corpress uses the Requests Python Library for HTTP requests. You can pass headers you want in HTTP requests directly as a dict. See documentation here. The most relevant one is to set a User-Agent header. See the note below about how to set an appropriate User-Agent.\n* params: The posts and pages endpoints support a number of parameters. This includes parameters to specify a search term, restrict dates and set the way results are ordered. Set additional parameters as a dict. See the Requests library documentation on passing parameters in URLS to understand this. * max_pages: By default Corpress will collect all post (or pages). That might not be necessary. Interpret max_pages as the maximum number of successful API requests. The REST API normally returns 10 posts/pages per request, so if you want 100 posts you would set max_pages to 10.\n\nSet an appropriate User-Agent\nHere’s a suggested format: Your Research Project (https://university.edu/webpage). See how to set this below.\n\nseconds_between_requests = 5\nheaders = {'User-Agent': 'Your Research Project (https://university.edu/webpage)'}\nparams = {'search': 'common'} # just comment out or remove this line to collect every post, I've just chosen a word arbitrarily here\nmax_pages = None # collecting all available data, if want less data - set to an integer\n\nNow you can call the corpress function and create a corpus. There will be lots of information logged about collecting and processing the texts. When completed it will output a table with a summary of the process and texts collected. This is the same data returned by the corpress function.\n\nresult = corpress(url=url, \n                  endpoint_type=endpoint_type, \n                  corpus_format=corpus_format, \n                  json_save_path=json_save_path, \n                  corpus_save_path=corpus_save_path, \n                  csv_save_file=csv_save_file, \n                  include_title_in_text=include_title_in_text, \n                  seconds_between_requests=seconds_between_requests, \n                  headers=headers, \n                  params=params, \n                  max_pages=max_pages)\n\n2024-08-23 11:21:25 - INFO - Found REST API endpoint link\n2024-08-23 11:21:25 - INFO - Setting posts route https://adho.org/wp-json/wp/v2/posts\n2024-08-23 11:21:25 - INFO - Using JSON save path: ../test_data/example/json/\n2024-08-23 11:21:27 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=1\n2024-08-23 11:21:27 - INFO - Total pages to retrieve is 3\n2024-08-23 11:21:34 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=2\n2024-08-23 11:21:40 - INFO - Downloading https://adho.org/wp-json/wp/v2/posts?search=common&page=3\n2024-08-23 11:21:45 - INFO - Creating corpus in txt format\n2024-08-23 11:21:45 - INFO - Using corpus save path: ../test_data/example/txt/\n2024-08-23 11:21:45 - INFO - Creating CSV file for metadata: ../test_data/example/metadata.csv\n2024-08-23 11:21:45 - INFO - Processing JSON: posts-3.json\n2024-08-23 11:21:45 - INFO - Processing JSON: posts-2.json\n2024-08-23 11:21:45 - INFO - Processing JSON: posts-1.json\n\n\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\n0\nurl\nhttps://www.adho.org/\n\n\n1\nendpoint_url\nhttps://adho.org/wp-json/wp/v2/posts\n\n\n2\nheaders\n{'User-Agent': 'Your Research Project (https:/...\n\n\n3\nparams\n{'search': 'common'}\n\n\n4\nget_api_url\nTrue\n\n\n5\nget_json\nTrue\n\n\n6\ncreate_corpus\nTrue\n\n\n7\ncorpus_format\ntxt\n\n\n8\ncorpus_save_path\n../test_data/example/txt/\n\n\n9\ncsv_save_file\n../test_data/example/metadata.csv\n\n\n10\ncorpus_texts_count\n29\n\n\n\n\n\n\n\nYou can now preview the data you’ve collected.\n\nimport pandas as pd\npd.set_option('display.max_colwidth', None) # to display full text in pandas dataframe\nmetadata = pd.read_csv(csv_save_file)\nmetadata = metadata.sort_values('date')\nmetadata[['date', 'link', 'title', 'filename']].head(5) # display first 5 rows of metadata, this is not all the fields available\n\n\n\n\n\n\n\n\ndate\nlink\ntitle\nfilename\n\n\n\n\n8\n2012-12-06\nADHO Adopts Creative Commons License for Its Web Site\nhttps://adho.org/2012/12/06/adho-adopts-creative-commons-license-for-its-web-site/\n2012-12-06-post-382-adho-adopts-creative-commons-license-for-its-web-site.txt\n\n\n7\n2013-03-28\nApply to be ADHO’s Publications Liaison\nhttps://adho.org/2013/03/28/apply-to-be-adhos-publications-liaison/\n2013-03-28-post-366-apply-to-be-adhos-publications-liaison.txt\n\n\n6\n2013-06-23\nADHO Calls for Proposals for New Special Interest Groups\nhttps://adho.org/2013/06/23/adho-calls-for-proposals-for-new-special-interest-groups/\n2013-06-23-post-338-adho-calls-for-proposals-for-new-special-interest-groups.txt\n\n\n5\n2013-07-09\nParticipate in the Joint ADHO and centerNet AGM at Digital Humanities 2013\nhttps://adho.org/2013/07/09/participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013/\n2013-07-09-post-408-participate-in-the-joint-adho-and-centernet-agm-at-digital-humanities-2013.txt\n\n\n4\n2013-07-14\nDigital Humanities 2015 to be held in Sydney, Australia\nhttps://adho.org/2013/07/14/digital-humanities-2015-to-be-held-in-sydney-australia/\n2013-07-14-post-288-digital-humanities-2015-to-be-held-in-sydney-australia.txt\n\n\n\n\n\n\n\nYou can view a specific text file (if you used the ‘txt’ format) like this:\n\nimport os\nfilename = '2012-12-06-post-382-adho-adopts-creative-commons-license-for-its-web-site.txt'\nwith open(os.path.join(corpus_save_path, filename), 'r', encoding = 'utf-8') as file:\n    text = file.read()   \n    print(text)\n\nADHO Adopts Creative Commons License for Its Web Site\n\nThe Alliance of Digital Humanities Organizations (ADHO) is pleased to announce that all content on its web site is now available under a Creative Commons Attribution (CC-BY) license. This means that individuals and organizations are welcome to re-use and adapt ADHO’s documents and resources, so long as ADHO is cited as the source. Neil Fraistat, Chair of ADHO’s Steering Committee, notes that “this is one of an ongoing series of actions this year that are being designed to make ADHO resources more open and available to the larger community.”\n \nADHO’s decision to adopt the CC-BY license was prompted by the recognition that through explicitly sharing its work it can have a greater impact, contribute to best practices, and demonstrate its support for open access. Recently the Program Committee for the 2013 Digital Humanities conference  revamped ADHO’s Guidelines for Proposal Authors & Reviewers, making them more inclusive, concrete, and transparent. PC chair Bethany Nowviskie received a request from the organizers of another conference to re-use these guidelines. Prompted by Nowviskie's suggestion, the ADHO Steering Committee determined that not only should the conference guidelines be made freely available, but its entire web site.\n \nIn adopting a Creative Commons license for its website, ADHO follows suit with several of its existing publications, including Digital Studies/Le Champ Numerique, Digital Humanities Quarterly, and DH Answers.",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#collecting-both-posts-and-pages",
    "href": "index.html#collecting-both-posts-and-pages",
    "title": "corpress",
    "section": "Collecting both posts and pages",
    "text": "Collecting both posts and pages\nIf you want to collect both posts and pages, just invoke corpress twice: once with endpoint_type set to ‘posts’ and then with it set to ‘pages’.\nIf you are outputting in the ‘txt’ corpus format without a metadata file (i.e. csv_save_file set to None or omitted from the function call), you won’t have a problem. The filenames for posts/pages won’t conflict.\nIf you are specifying a csv_save_file - either because you are outputting in the ‘csv’ corpus format or in the ‘txt’ format and wanting the meta-data - make sure you use a separate csv_save_file for ‘posts’ and ‘pages’. You will get two separate files, combining these with a library like Pandas, which is installed with Corpress, is trivial. I will leave that for you to Google how to merge two CSV files into one using Pandas.",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "index.html#no-rest-api-available",
    "href": "index.html#no-rest-api-available",
    "title": "corpress",
    "section": "No REST API available",
    "text": "No REST API available\nHere’s an example showing what you will see if there no REST API is accessible.\n\n# test a site that has no endpoint\nresult = corpress(url = 'https://www.whitehouse.gov/', \n                endpoint_type='posts',\n                corpus_format='txt',\n                json_save_path = '../test_data/json/', \n                corpus_save_path = '../test_data/corpus/', \n                max_pages=2)\n\n2024-08-23 11:21:46 - INFO - No REST API endpoint link in markup\n2024-08-23 11:21:46 - INFO - Guessing posts route based on URL https://www.whitehouse.gov/wp-json/wp/v2/posts\n2024-08-23 11:21:46 - INFO - Using JSON save path: ../test_data/json/\n2024-08-23 11:21:46 - INFO - Max pages to retrieve from API is set: 2\n2024-08-23 11:21:47 - INFO - Downloading https://www.whitehouse.gov/wp-json/wp/v2/posts?page=1\n2024-08-23 11:21:47 - ERROR - Error downloading page 1 from https://www.whitehouse.gov/wp-json/wp/v2/posts\n2024-08-23 11:21:47 - ERROR - Status code: 403\n2024-08-23 11:21:47 - ERROR - It appears that this website does not provide access to the REST API. Exiting.\n2024-08-23 11:21:47 - ERROR - Error downloading data. Exiting.\n\n\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\n0\nurl\nhttps://www.whitehouse.gov/\n\n\n1\nendpoint_url\nhttps://www.whitehouse.gov/wp-json/wp/v2/posts\n\n\n2\nheaders\nNone\n\n\n3\nparams\nNone\n\n\n4\nget_api_url\nTrue\n\n\n5\nget_json\nFalse\n\n\n6\ncreate_corpus\nFalse\n\n\n7\ncorpus_format\ntxt\n\n\n8\ncorpus_save_path\n../test_data/corpus/\n\n\n9\ncsv_save_file\nNone\n\n\n10\ncorpus_texts_count\n0",
    "crumbs": [
      "corpress"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nget_api_url\n\n get_api_url (url:str, endpoint_type:str='posts', headers:dict=None)\n\nQueries a URL to get the REST API route for the endpoint type provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nthe URL of the WordPress website\n\n\nendpoint_type\nstr\nposts\nposts or pages\n\n\nheaders\ndict\nNone\noptional headers for requests\n\n\n\n\nsource\n\n\nget_json\n\n get_json (endpoint_url:str, endpoint_type:str='posts', headers:dict=None,\n           params:dict=None, json_save_path:str=None,\n           seconds_between_requests:int=5, max_pages:int=None)\n\nDownload and save JSON data from a specific REST API endpoint.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nendpoint_url\nstr\n\nthe URL of the WordPress REST API endpoint\n\n\nendpoint_type\nstr\nposts\nthe type of data to download\n\n\nheaders\ndict\nNone\noptional headers for requests\n\n\nparams\ndict\nNone\noptional parameters to pass to the API\n\n\njson_save_path\nstr\nNone\npath to save the JSON data\n\n\nseconds_between_requests\nint\n5\nnumber of seconds to wait between requests, must be at least 1\n\n\nmax_pages\nint\nNone\nmaximum number of pages to download\n\n\nReturns\nbool\n\nTrue if successful, False otherwise\n\n\n\n\nsource\n\n\ncreate_corpus\n\n create_corpus (corpus_format:str='txt', json_save_path:str=None,\n                corpus_save_path:str=None, csv_save_file:str=None,\n                include_title_in_text:bool=True)\n\nCreate a corpus from downloaded JSON data in txt or csv format.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncorpus_format\nstr\ntxt\nformat of the corpus files, txt or csv\n\n\njson_save_path\nstr\nNone\npath to JSON data\n\n\ncorpus_save_path\nstr\nNone\npath to save corpus in txt format\n\n\ncsv_save_file\nstr\nNone\npath to CSV file to output corpus in CSV format (or metadata if txt corpus)\n\n\ninclude_title_in_text\nbool\nTrue\ninclude the title in the text file\n\n\nReturns\nbool\n\nTrue if successful, False if there are errors parsing the JSON\n\n\n\n\nsource\n\n\nresult_reporting\n\n result_reporting (result:dict, output:bool=True)\n\nOutputs the results of the corpress process\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresult\ndict\n\nthe result dictionary\n\n\noutput\nbool\nTrue\noutput the results\n\n\nReturns\ndict\n\nreturns the result dictionary\n\n\n\n\nsource\n\n\ncorpress\n\n corpress (url:str, endpoint_type:str='posts', headers:dict=None,\n           params:dict=None, corpus_format:str='txt',\n           json_save_path:str=None, corpus_save_path:str=None,\n           csv_save_file:str=None, seconds_between_requests:int=5,\n           max_pages:int=None, include_title_in_text:bool=True,\n           output:bool=True)\n\nRetrieve data from the REST API and create a corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nthe URL of the WordPress website\n\n\nendpoint_type\nstr\nposts\nposts or pages\n\n\nheaders\ndict\nNone\noptional headers for requests\n\n\nparams\ndict\nNone\noptional parameters to pass to the API\n\n\ncorpus_format\nstr\ntxt\nformat of the corpus files, txt or csv\n\n\njson_save_path\nstr\nNone\npath to save the JSON data\n\n\ncorpus_save_path\nstr\nNone\npath to save the corpus in txt format\n\n\ncsv_save_file\nstr\nNone\npath to CSV file to output corpus in CSV format (or metadata if txt corpus)\n\n\nseconds_between_requests\nint\n5\nnumber of seconds to wait between requests\n\n\nmax_pages\nint\nNone\nmaximum number of pages to download\n\n\ninclude_title_in_text\nbool\nTrue\noption to include the title in the text file\n\n\noutput\nbool\nTrue\noption to output the results of the process\n\n\nReturns\ndict\n\ndictionary with results of each stage of the process and the number of texts in the corpus",
    "crumbs": [
      "core"
    ]
  }
]